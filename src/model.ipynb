{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np  \n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model: CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Load the model\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_combined_size: 517\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV\n",
    "\n",
    "data_frame = pd.read_csv('/home/guimcc/OneDrive/General/Projectes/HackUPC2024/index/images_2.csv')\n",
    "\n",
    "# Fit encoder to define the size\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoder.fit(data_frame[['season','category','type']])\n",
    "\n",
    "max_combined_size = 512 + sum(len(categories) for categories in encoder.categories_)\n",
    "\n",
    "print(f\"max_combined_size: {max_combined_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image_path = '/home/guimcc/OneDrive/General/Projectes/HackUPC2024/images_2'\n",
    "h5pt_file_path = '../ckp/images_2.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the processing of every image\n",
    "def process_and_combine_data(data_row, device, model, preprocess, encoder):\n",
    "    \n",
    "    relative_image_path = data_row['path']  # Assuming 'path' column has relative paths or filenames\n",
    "    full_image_path = os.path.join(base_image_path, relative_image_path)\n",
    "    try:\n",
    "        image = preprocess(Image.open(full_image_path)).unsqueeze(0).to(device) # Add the batched image to the device\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image).cpu().numpy() # Extract features from the image (Embedding)\n",
    "\n",
    "        # Assuming 'category' is a column in your DataFrame with categorical data\n",
    "        categorical_data = [data_row[['season', 'category', 'type']].values.tolist()]\n",
    "        one_hot_features = encoder.transform(categorical_data)\n",
    "\n",
    "        # Combine image features with one-hot encoded features\n",
    "        combined_features = np.concatenate((image_features, one_hot_features), axis=1)\n",
    "        return combined_features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process image {full_image_path}: {str(e)}\")\n",
    "        print(data_row)\n",
    "        print(categorical_data)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 252/252 [00:10<00:00, 24.42it/s] \n"
     ]
    }
   ],
   "source": [
    "with h5py.File(h5pt_file_path, 'w') as h5f:\n",
    "    \n",
    "    # Create the dataset\n",
    "    dset = h5f.create_dataset(\"image_embeddings\", shape=(0, max_combined_size), maxshape=(None, max_combined_size), dtype='float32')\n",
    "    \n",
    "    \n",
    "    for index, row in tqdm(data_frame.iterrows(), total=len(data_frame), desc=\"Processing images\"):\n",
    "        result = process_and_combine_data(row, device, model, preprocess, encoder)\n",
    "        if result is not None:\n",
    "            dset.resize(dset.shape[0]+1, axis=0)\n",
    "            dset[-1] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File(h5pt_file_path, 'r') as file:\n",
    "#     train_embeddings = file['image_embeddings']\n",
    "\n",
    "h5f = h5py.File(h5pt_file_path, 'r')\n",
    "train_embeddings = h5f['image_embeddings'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the model's weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find closest data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/home/guimcc/OneDrive/General/Projectes/HackUPC2024/images/images_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combined_embedding(relative_image_path, categorical_data, model, preprocess, encoder, device):\n",
    "    # Process the image\n",
    "    full_image_path = os.path.join(test_dir, relative_image_path)\n",
    "    image = preprocess(Image.open(full_image_path)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_embedding = model.encode_image(image).cpu().numpy()\n",
    "    \n",
    "    # One-hot encode the categorical data\n",
    "    # Ensure categorical_data is in the form of a 2D array [[cat1, cat2, ..., catN]]\n",
    "    one_hot_features = encoder.transform([categorical_data])\n",
    "    \n",
    "    # Combine the image embedding and one-hot features\n",
    "    combined_embedding = np.concatenate((image_embedding.squeeze(0), one_hot_features.squeeze(0)), axis=0)\n",
    "    \n",
    "    return combined_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embedding_cosine(new_embedding, embeddings):\n",
    "    similarities = np.array([cosine(new_embedding, emb) for emb in embeddings])\n",
    "    closest_index = np.argmin(similarities)\n",
    "    return closest_index, similarities[closest_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(img_path1, img_path2, title1=\"Test Image\", title2=\"Closest Match\"):\n",
    "    img1 = Image.open(img_path1)\n",
    "    img2 = Image.open(img_path2)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img1)\n",
    "    plt.title(title1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img2)\n",
    "    plt.title(title2)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test images:  14%|█▎        | 3/22 [00:00<00:01, 10.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 w path 2007_0_2024_W_0_1.jpeg : Closest index: 689 w similarity: 0.05755240964892805 path: 246_1_2024_V_0_1\n",
      " 1 w path 2000_1_2024_V_0_2.jpeg : Closest index: 432 w similarity: 0.051806041358309574 path: 154_0_2024_V_0_2\n",
      " 2 w path 2006_2_2024_V_0_2.jpeg : Closest index: 242 w similarity: 0.10488691111555581 path: 86_2_2023_I_0_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test images:  32%|███▏      | 7/22 [00:00<00:01, 11.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3 w path 2007_1_2024_W_0_1.jpeg : Closest index: 1097 w similarity: 0.059268646421439586 path: 400_1_2023_I_0_1\n",
      " 4 w path 2003_2_2024_V_0_3.jpeg : Closest index: 458 w similarity: 0.10829283342717944 path: 162_2_2024_V_0_3\n",
      " 5 w path 2001_2_2023_I_0_1.jpeg : Closest index: 825 w similarity: 0.05718575111979762 path: 302_0_2023_I_0_2\n",
      " 6 w path 2001_1_2024_V_0_1.jpeg : Closest index: 947 w similarity: 0.14309539668047377 path: 346_1_2023_I_0_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test images:  41%|████      | 9/22 [00:00<00:01, 11.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7 w path 2004_2_2024_V_0_3.jpeg : Closest index: 458 w similarity: 0.1121741889375012 path: 162_2_2024_V_0_3\n",
      " 8 w path 2000_2_2024_V_0_2.jpeg : Closest index: 434 w similarity: 0.05674107029754971 path: 154_2_2024_V_0_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test images:  50%|█████     | 11/22 [00:01<00:01,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9 w path 2006_0_2024_V_0_2.jpeg : Closest index: 991 w similarity: 0.0706089920893892 path: 363_0_2024_W_0_2\n",
      " 10 w path 2005_1_2024_V_0_1.jpeg : Closest index: 307 w similarity: 0.12008069176767178 path: 109_1_2024_V_0_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test images:  59%|█████▉    | 13/22 [00:01<00:00,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11 w path 2003_0_2024_V_0_3.jpeg : Closest index: 456 w similarity: 0.12889154116746404 path: 162_0_2024_V_0_3\n",
      " 12 w path 2002_1_2024_V_1_1.jpeg : Closest index: 268 w similarity: 0.19635429099440682 path: 95_1_2024_V_1_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test images:  68%|██████▊   | 15/22 [00:01<00:00,  7.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13 w path 2000_0_2024_V_0_2.jpeg : Closest index: 432 w similarity: 0.04960910499488069 path: 154_0_2024_V_0_2\n",
      " 14 w path 2004_0_2024_V_0_3.jpeg : Closest index: 629 w similarity: 0.0943853930426084 path: 225_1_2024_V_0_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test images:  82%|████████▏ | 18/22 [00:02<00:00,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15 w path 2005_2_2024_V_0_1.jpeg : Closest index: 360 w similarity: 0.02206120380359944 path: 127_2_2024_V_0_1\n",
      " 16 w path 2004_1_2024_V_0_3.jpeg : Closest index: 456 w similarity: 0.17966709639554923 path: 162_0_2024_V_0_3\n",
      " 17 w path 2001_0_2024_V_0_1.jpeg : Closest index: 947 w similarity: 0.13122329758273898 path: 346_1_2023_I_0_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test images: 100%|██████████| 22/22 [00:02<00:00,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18 w path 2005_0_2024_V_0_1.jpeg : Closest index: 307 w similarity: 0.14234904679785587 path: 109_1_2024_V_0_1\n",
      " 19 w path 2003_1_2024_V_0_3.jpeg : Closest index: 728 w similarity: 0.1140681842794623 path: 260_0_2024_V_0_3\n",
      " 20 w path 2006_1_2024_V_0_2.jpeg : Closest index: 724 w similarity: 0.0575025952358591 path: 258_1_2024_V_0_2\n",
      " 21 w path 2002_0_2024_V_1_1.jpeg : Closest index: 1187 w similarity: 0.12435205110949943 path: 440_0_2024_V_1_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data_frame_test = pd.read_csv('/home/guimcc/OneDrive/General/Projectes/HackUPC2024/index/images_resized_test.csv')\n",
    "\n",
    "# Get the data_frame_test\n",
    "\n",
    "for index, row in tqdm(data_frame_test.iterrows(), total=len(data_frame_test), desc=\"Processing test images\"):\n",
    "    \n",
    "    new_image_path = row['path']  # Adjust the column name as necessary\n",
    "    categorical_data = row[['season', 'category', 'type']].tolist()\n",
    "    \n",
    "    new_embedding = generate_combined_embedding(new_image_path, categorical_data, model, preprocess, encoder, device)\n",
    "\n",
    "    closest_index, similarity = find_closest_embedding_cosine(new_embedding, train_embeddings)\n",
    "    \n",
    "    print(f\" {index} w path {new_image_path} : Closest index: {closest_index} w similarity: {similarity} path: {data_frame.loc[closest_index, 'path']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1187\n"
     ]
    }
   ],
   "source": [
    "print(closest_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "Outline of the overall method of retrieveng images and obtaining their embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [preprocess(Image.open(image_path)).unsqueeze(0).to('cuda') for image_path in image_paths]\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features_l = [model.encode_image(image) for image in images]\n",
    "\n",
    "image_features = torch.stack(image_features_l).squeeze()\n",
    "\n",
    "tabular_data = np.array([\n",
    "    ['m', 's', 'v'],\n",
    "    ['m', 's', 'v'],\n",
    "    ['m', 'd', 'v'],\n",
    "    ['m', 's', 'v'],\n",
    "])\n",
    "\n",
    "weight = 10.0\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoded_categorical = encoder.fit_transform(tabular_data).toarray()\n",
    "encoded_categorical = torch.tensor(encoded_categorical, device='cuda').float()\n",
    "\n",
    "combined_features = torch.cat((image_features, weight*encoded_categorical), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data = np.array([\n",
    "    ['m', 's', 'v'],\n",
    "    ['m', 's', 'v'],\n",
    "    ['m', 'd', 'v'],\n",
    "    ['m', 's', 'v'],\n",
    "])\n",
    "\n",
    "weight = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "encoded_categorical = encoder.fit_transform(tabular_data).toarray()\n",
    "encoded_categorical = torch.tensor(encoded_categorical, device='cuda').float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = torch.cat((image_features, weight*encoded_categorical), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "with h5py.File('combined_embeddings.hdf5', 'w') as f:\n",
    "    f.create_dataset('embeddings', data=combined_features.cpu().detach().numpy())\n",
    "\n",
    "# Load embeddings\n",
    "with h5py.File('combined_embeddings.hdf5', 'r') as f:\n",
    "    loaded_embeddings = f['embeddings'][:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
